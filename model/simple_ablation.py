import argparse
import numpy as np
import pandas as pd
import tensorflow as tf
from math import radians
from datetime import datetime
import matplotlib.pyplot as plt
from tensorflow.keras import backend as K
from tensorflow.keras import Model
from tensorflow.keras.layers import Layer
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Masking
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Concatenate
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import initializers, regularizers, constraints
import tensorflow_addons as tfa
from cond_rnn import ConditionalRecurrent
from sklearn.metrics.pairwise import haversine_distances
from itertools import combinations
import os

os.environ["CUDA_VISIBLE_DEVICES"] = '0'
config = tf.compat.v1.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.9
config.gpu_options.allow_growth = True
sess = tf.compat.v1.Session(config=config)

# æ¶ˆèå®éªŒé…ç½®
ablation_configs = {
    "CBAM_full": {
        "use_dp_condition": True,
        "use_attention": True,
        "use_weather": True,
        "use_bilstm": True,
        "dp_concatenate": False
    }
    "CBAM_no_dp_condition": {
        "use_dp_condition": False,
        "use_attention": True,
        "use_weather": True,
        "use_bilstm": True,
        "dp_concatenate": False
    },
    "CBAM_dp_concatenate": {
        "use_dp_condition": False,
        "use_attention": True,
        "use_weather": True,
        "use_bilstm": True,
        "dp_concatenate": True
    },
    "CBAM_no_attention": {
        "use_dp_condition": True,
        "use_attention": False,
        "use_weather": True,
        "use_bilstm": True,
        "dp_concatenate": False
    },
    "CBAM_no_weather": {
        "use_dp_condition": True,
        "use_attention": True,
        "use_weather": False,
        "use_bilstm": True,
        "dp_concatenate": False
    },
    "CBAM_unidirectional": {
        "use_dp_condition": True,
        "use_attention": True,
        "use_weather": True,
        "use_bilstm": False,
        "dp_concatenate": False
    },
}

# å…¨å±€å‚æ•°
max_traj_len = 81
extra_dim_full = 9  # å®Œæ•´ç‰¹å¾ç»´åº¦
extra_dim_no_weather = 3  # æ— å¤©æ°”ç‰¹å¾ï¼šdaytype, hour_sin, hour_cos
dp_dim = 3
level = 8
lstm_units = 256
dense_extra_units = 18
dense_concat_units = 1024


class attention2(Layer):
    def __init__(self, step_dim,
                 W_regularizer=None, b_regularizer=None,
                 W_constraint=None, b_constraint=None,
                 bias=True, **kwargs):

        self.supports_masking = True
        self.init = initializers.get('glorot_uniform')
        self.W_regularizer = regularizers.get(W_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)
        self.W_constraint = constraints.get(W_constraint)
        self.b_constraint = constraints.get(b_constraint)
        self.bias = bias
        self.step_dim = step_dim
        self.features_dim = 0
        super(attention2, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) == 3
        self.W = self.add_weight(shape=(input_shape[-1],),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        self.features_dim = input_shape[-1]

        if self.bias:
            self.b = self.add_weight(shape=(input_shape[1],),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)
        else:
            self.b = None
        self.built = True

    def compute_mask(self, input, input_mask=None):
        return None

    def call(self, x, mask=None):
        features_dim = self.features_dim
        step_dim = self.step_dim

        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))
        if self.bias:
            e += self.b
        e = K.tanh(e)

        a = K.exp(e)
        if mask is not None:
            a *= K.cast(mask, K.floatx())
        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())
        a = K.expand_dims(a)

        c = K.sum(a * x, axis=1)
        return c

    def compute_output_shape(self, input_shape):
        return input_shape[0], self.features_dim

    def get_config(self):
        config = super().get_config().copy()
        config.update({
            'step_dim': self.step_dim,
            'W_regularizer': self.W_regularizer,
            'b_regularizer': self.b_regularizer,
            'W_constraint': self.W_constraint,
            'b_constraint': self.b_constraint,
            'bias': self.bias
        })
        return config


def load_train_data(per=None):
    train_data = pd.read_csv("../training data/" + per + "/train_" + per + "_all_level8_new_clusters_1.csv")
    val_data = pd.read_csv("../validation data/" + per + "/val_" + per + "_all_level8_new_clusters_1.csv")
    return train_data, val_data


def load_test_data(per):
    test_data = pd.read_csv("../testing data/" + per + "0%/test_" + per + "0%_all_level8_new_clusters_1.csv")
    return test_data


def build_ablation_model(config):
    """æ ¹æ®æ¶ˆèå®éªŒé…ç½®æ„å»ºæ¨¡å‹"""
    input_gps = Input(shape=(max_traj_len, 2), name='input_gps')
    input_dp = Input(shape=(dp_dim,), name='input_dp')

    # æ ¹æ®é…ç½®ç¡®å®šç‰¹å¾ç»´åº¦
    if config["use_weather"]:
        extra_dim = extra_dim_full
        input_extra = Input(shape=(extra_dim,), name='input_extra')
    else:
        extra_dim = extra_dim_no_weather
        input_extra = Input(shape=(extra_dim,), name='input_extra')

    # åŠ¨æ€ä»·æ ¼å¤„ç†æ–¹å¼
    if config["use_dp_condition"]:
        # æ¡ä»¶å¾ªç¯æœºåˆ¶ï¼ˆåŸå§‹CBAMï¼‰
        forward_lstm_gps = ConditionalRecurrent(
            LSTM(lstm_units, return_sequences=True, kernel_regularizer=tf.keras.regularizers.L2(0.01)))
        backward_lstm_gps = ConditionalRecurrent(
            LSTM(lstm_units, return_sequences=True, kernel_regularizer=tf.keras.regularizers.L2(0.01),
                 go_backwards=True))
        bilstm_inputs = (input_gps, input_dp)
    else:
        # æ™®é€šLSTM
        forward_lstm_gps = LSTM(lstm_units, return_sequences=True, kernel_regularizer=tf.keras.regularizers.L2(0.01))
        backward_lstm_gps = LSTM(lstm_units, return_sequences=True, kernel_regularizer=tf.keras.regularizers.L2(0.01),
                                 go_backwards=True)
        bilstm_inputs = input_gps

    # BiLSTMæˆ–å•å‘LSTM
    if config["use_bilstm"]:
        bilstm_gps = Bidirectional(layer=forward_lstm_gps, backward_layer=backward_lstm_gps)(inputs=bilstm_inputs)
    else:
        bilstm_gps = forward_lstm_gps(inputs=bilstm_inputs)

    # æ³¨æ„åŠ›æœºåˆ¶
    if config["use_attention"]:
        attention_output = attention2(max_traj_len)(bilstm_gps)
    else:
        # æ— æ³¨æ„åŠ›ï¼šä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        attention_output = bilstm_gps[:, -1, :]

    # è¾…åŠ©ç‰¹å¾å¤„ç†
    dense_extra = Dense(dense_extra_units, activation='relu')(input_extra)

    # æ‹¼æ¥å±‚ - å¤„ç†åŠ¨æ€ä»·æ ¼æ‹¼æ¥æƒ…å†µ
    if config["dp_concatenate"]:
        # åŠ¨æ€ä»·æ ¼ç›´æ¥æ‹¼æ¥
        concat = Concatenate()([attention_output, dense_extra, input_dp])
    else:
        # åŸå§‹æ‹¼æ¥æ–¹å¼
        concat = Concatenate()([attention_output, dense_extra])

    # è¾“å‡ºå±‚
    dense_concat = Dense(dense_concat_units, activation='relu')(concat)
    dropout_concat = Dropout(0.5)(dense_concat)
    softmax_concat = Dense(965, activation='softmax')(dropout_concat)

    # æ„å»ºæ¨¡å‹
    if config["use_dp_condition"] or config["dp_concatenate"]:
        model = Model(inputs=[input_gps, input_dp, input_extra], outputs=softmax_concat)
    else:
        model = Model(inputs=[input_gps, input_extra], outputs=softmax_concat)

    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                  metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5),
                           tfa.metrics.F1Score(num_classes=965, average='macro')])
    return model


def normalize(data, level):
    for i in range(len(data)):
        data[i] = [[k[0] / pow(2, level), k[1] / pow(2, level)] for k in data[i]]
    return data


def coords_padding(data, maxlen):
    padded_data = []
    for i in range(len(data)):
        traj = data[i]
        if len(traj) < maxlen:
            traj = traj + [[0, 0]] * (maxlen - len(traj))
        elif len(traj) > maxlen:
            traj = traj[:maxlen]
        traj = [[item[0], item[1]] if len(item) == 2 else [0, 0] for item in traj]
        padded_data.append(traj)
    return padded_data


def to_one_hot(data):
    return to_categorical(data, num_classes=965)


def top_k_avg_dist(logits, targets, k, center_clusters):
    values, indices = tf.math.top_k(logits, k=k, sorted=True)
    all_dist = []
    for i in range(len(indices)):
        tmp_dist = []
        for pred in indices[i]:
            pred_lon = radians(center_clusters.iloc[int(pred)]['center_lon'])
            pred_lat = radians(center_clusters.iloc[int(pred)]['center_lat'])
            true_lon = radians(center_clusters.iloc[int(targets[i])]['center_lon'])
            true_lat = radians(center_clusters.iloc[int(targets[i])]['center_lat'])
            tmp_dist.append(
                (haversine_distances([[pred_lat, pred_lon], [true_lat, true_lon]]) * 6371000 / 1000)[0][1] * 1000)
        all_dist.append(tmp_dist)
    all_dist = np.array(all_dist)
    return np.mean(all_dist), np.mean(np.median(all_dist, axis=1)), np.mean(np.std(all_dist, axis=1))


def train_single_ablation_model(config_name, config):
    """è®­ç»ƒå•ä¸ªæ¶ˆèå®éªŒæ¨¡å‹"""
    print(f"Training {config_name}...")

    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs(f"ablation_checkpoints/{config_name}", exist_ok=True)

    # åŠ è½½æ•°æ®
    train_data, val_data = load_train_data("80%")

    # æ•°æ®é¢„å¤„ç†
    train_x_gps = [eval(i) for i in train_data['coords_of_traj']]
    train_y = np.array(to_one_hot(train_data['dest_cluster']))
    val_x_gps = [eval(i) for i in val_data['coords_of_traj']]
    val_y = np.array(to_one_hot(val_data['dest_cluster']))

    train_x_gps = normalize(train_x_gps, level)
    val_x_gps = normalize(val_x_gps, level)
    train_x_gps = np.array(coords_padding(train_x_gps, max_traj_len))
    val_x_gps = np.array(coords_padding(val_x_gps, max_traj_len))

    # ç‰¹å¾é€‰æ‹©
    if config["use_weather"]:
        train_x_extra = np.array(train_data[['daytype', 'weather_0', 'weather_1', 'weather_2',
                                             'weather_3', 'weather_4', 'weather_5', 'hour_sin', 'hour_cos']])
        val_x_extra = np.array(val_data[['daytype', 'weather_0', 'weather_1', 'weather_2',
                                         'weather_3', 'weather_4', 'weather_5', 'hour_sin', 'hour_cos']])
    else:
        train_x_extra = np.array(train_data[['daytype', 'hour_sin', 'hour_cos']])
        val_x_extra = np.array(val_data[['daytype', 'hour_sin', 'hour_cos']])

    train_x_dp = np.array(train_data[['dp_cur', 'dp_30min_prev', 'dp_1h_prev']])
    val_x_dp = np.array(val_data[['dp_cur', 'dp_30min_prev', 'dp_1h_prev']])

    # æ„å»ºæ¨¡å‹
    model = build_ablation_model(config)

    # å›è°ƒå‡½æ•°
    checkpoint_path = f"ablation_checkpoints/{config_name}/checkpoint"
    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_path,
        save_weights_only=True,
        monitor='val_top_k_categorical_accuracy',
        mode='max',
        save_best_only=True
    )

    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )

    # è®­ç»ƒ
    print(f"Starting training for {config_name}...")
    if config["use_dp_condition"] or config["dp_concatenate"]:
        history = model.fit(
            [train_x_gps, train_x_dp, train_x_extra], train_y,
            epochs=50,  # å¯ä»¥å…ˆè¯•50ä¸ªepochs
            batch_size=128,
            validation_data=([val_x_gps, val_x_dp, val_x_extra], val_y),
            callbacks=[model_checkpoint, early_stopping],
            verbose=1
        )
    else:
        history = model.fit(
            [train_x_gps, train_x_extra], train_y,
            epochs=50,
            batch_size=128,
            validation_data=([val_x_gps, val_x_extra], val_y),
            callbacks=[model_checkpoint, early_stopping],
            verbose=1
        )

    print(f"âœ… Finished training {config_name}")
    return history


def run_ablation_study():
    """è¿è¡Œæ‰€æœ‰æ¶ˆèå®éªŒ - ä¿®æ­£ç‰ˆæœ¬"""
    center_clusters_gps = pd.read_csv(
        "../processed_data/clusters_5_ring/mean_shift clustering/965_clusters_center_coords.csv")

    all_results = {}

    for config_name, config in ablation_configs.items():
        print(f"\n{'=' * 50}")
        print(f"Running ablation study: {config_name}")
        print(f"{'=' * 50}")

        # æ„å»ºæ¨¡å‹
        model = build_ablation_model(config)

        # === ä¿®æ”¹è¿™é‡Œï¼šåŠ è½½å¯¹åº”é…ç½®çš„æƒé‡ ===
        checkpoint_path = f"ablation_checkpoints/{config_name}/checkpoint"
        try:
            model.load_weights(checkpoint_path)
            print(f"âœ… Loaded trained weights for {config_name}")
        except:
            print(f"âŒ No trained weights found for {config_name}")
            print(f"ğŸ’¡ Running training for {config_name} first...")

            # å¦‚æœæ²¡æœ‰è®­ç»ƒæƒé‡ï¼Œå…ˆè®­ç»ƒè¿™ä¸ªé…ç½®
            train_single_ablation_model(config_name, config)

            # é‡æ–°åŠ è½½æƒé‡
            try:
                model.load_weights(checkpoint_path)
                print(f"âœ… Loaded newly trained weights for {config_name}")
            except:
                print(f"âš ï¸  Using randomly initialized model for {config_name}")
        # === ä¿®æ”¹ç»“æŸ ===

        config_results = {}

        # æµ‹è¯•æ‰€æœ‰å‰ç¼€æ¯”ä¾‹
        for per in ['1', '2', '3', '4', '5', '6', '7', '8', '9']:
            print(f"Testing {per}0% prefix...")

            # åŠ è½½æµ‹è¯•æ•°æ®
            test_data = load_test_data(per)
            test_x_gps = [eval(i) for i in test_data['coords_of_traj']]
            test_y = np.array(test_data['dest_cluster'])

            # æ•°æ®é¢„å¤„ç†
            test_x_gps = normalize(test_x_gps, level)
            test_x_gps = np.array(coords_padding(test_x_gps, max_traj_len))

            # ç‰¹å¾é€‰æ‹©
            if config["use_weather"]:
                test_x_extra = np.array(test_data[['daytype', 'weather_0', 'weather_1', 'weather_2',
                                                   'weather_3', 'weather_4', 'weather_5', 'hour_sin', 'hour_cos']])
            else:
                test_x_extra = np.array(test_data[['daytype', 'hour_sin', 'hour_cos']])

            test_x_dp = np.array(test_data[['dp_cur', 'dp_30min_prev', 'dp_1h_prev']])

            # é¢„æµ‹
            if config["use_dp_condition"] or config["dp_concatenate"]:
                y_test_pred = model.predict([test_x_gps, test_x_dp, test_x_extra], batch_size=128, verbose=0)
            else:
                y_test_pred = model.predict([test_x_gps, test_x_extra], batch_size=128, verbose=0)

            # è®¡ç®—æŒ‡æ ‡
            k_results = {}
            for k in [1, 5, 10]:
                mean, avg_median, avg_std = top_k_avg_dist(y_test_pred, test_y, k, center_clusters=center_clusters_gps)
                k_results[f'Top-{k}'] = mean
                print(f"  Top-{k} Distance mean: {mean:.2f}m")

            config_results[f'{per}0%'] = k_results

        all_results[config_name] = config_results

        # ä¿å­˜æ¯ä¸ªé…ç½®çš„ç»“æœ
        with open(f"ablation_results_{config_name}.txt", "w") as f:
            f.write(f"Ablation Study Results: {config_name}\n")
            f.write("Config: " + str(config) + "\n\n")
            for prefix, metrics in config_results.items():
                f.write(f"{prefix}:\n")
                for metric, value in metrics.items():
                    f.write(f"  {metric}: {value:.2f}m\n")
                f.write("\n")

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    generate_comparison_report(all_results)

    return all_results



def generate_comparison_report(all_results):
    """ç”Ÿæˆæ¶ˆèå®éªŒå¯¹æ¯”æŠ¥å‘Š"""
    print(f"\n{'=' * 60}")
    print("ABLATION STUDY COMPARISON REPORT")
    print(f"{'=' * 60}")

    with open("ablation_comparison_report.txt", "w") as f:
        f.write("Ablation Study Comparison Report\n")
        f.write("=" * 50 + "\n\n")

        # æ¯”è¾ƒæ¯ä¸ªå‰ç¼€æ¯”ä¾‹ä¸‹å„é…ç½®çš„æ€§èƒ½
        prefixes = ['10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%']

        for prefix in prefixes:
            f.write(f"\n{prefix} Prefix Results:\n")
            f.write("-" * 30 + "\n")

            prefix_key = f"{prefix[0]}0%"
            results_at_prefix = {}

            for config_name, config_results in all_results.items():
                if prefix_key in config_results:
                    top1_error = config_results[prefix_key]['Top-1']
                    results_at_prefix[config_name] = top1_error

            # æŒ‰æ€§èƒ½æ’åº
            sorted_results = sorted(results_at_prefix.items(), key=lambda x: x[1])

            for i, (config_name, error) in enumerate(sorted_results):
                f.write(f"{i + 1:2d}. {config_name:<25} {error:8.2f}m\n")

            # è®¡ç®—ç›¸å¯¹äºå®Œæ•´æ¨¡å‹çš„æ€§èƒ½ä¸‹é™
            if "CBAM_full" in results_at_prefix:
                full_model_error = results_at_prefix["CBAM_full"]
                f.write(f"\nPerformance degradation relative to CBAM_full ({full_model_error:.2f}m):\n")
                for config_name, error in results_at_prefix.items():
                    if config_name != "CBAM_full":
                        degradation = ((error - full_model_error) / full_model_error) * 100
                        f.write(f"  {config_name:<25} +{degradation:6.2f}%\n")


def train_ablation_models():
    """è®­ç»ƒæ‰€æœ‰æ¶ˆèå®éªŒæ¨¡å‹ï¼ˆå¦‚æœéœ€è¦é‡æ–°è®­ç»ƒï¼‰"""
    for config_name, config in ablation_configs.items():
        print(f"\nTraining {config_name}...")

        # åŠ è½½æ•°æ®
        train_data, val_data = load_train_data("80%")

        # æ•°æ®é¢„å¤„ç†ï¼ˆæ ¹æ®é…ç½®é€‰æ‹©ç‰¹å¾ï¼‰
        train_x_gps = [eval(i) for i in train_data['coords_of_traj']]
        train_y = np.array(to_one_hot(train_data['dest_cluster']))
        val_x_gps = [eval(i) for i in val_data['coords_of_traj']]
        val_y = np.array(to_one_hot(val_data['dest_cluster']))

        train_x_gps = normalize(train_x_gps, level)
        val_x_gps = normalize(val_x_gps, level)
        train_x_gps = np.array(coords_padding(train_x_gps, max_traj_len))
        val_x_gps = np.array(coords_padding(val_x_gps, max_traj_len))

        # ç‰¹å¾é€‰æ‹©
        if config["use_weather"]:
            train_x_extra = np.array(train_data[['daytype', 'weather_0', 'weather_1', 'weather_2',
                                                 'weather_3', 'weather_4', 'weather_5', 'hour_sin', 'hour_cos']])
            val_x_extra = np.array(val_data[['daytype', 'weather_0', 'weather_1', 'weather_2',
                                             'weather_3', 'weather_4', 'weather_5', 'hour_sin', 'hour_cos']])
        else:
            train_x_extra = np.array(train_data[['daytype', 'hour_sin', 'hour_cos']])
            val_x_extra = np.array(val_data[['daytype', 'hour_sin', 'hour_cos']])

        train_x_dp = np.array(train_data[['dp_cur', 'dp_30min_prev', 'dp_1h_prev']])
        val_x_dp = np.array(val_data[['dp_cur', 'dp_30min_prev', 'dp_1h_prev']])

        # æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
        model = build_ablation_model(config)

        checkpoint_path = f"ablation_checkpoints/{config_name}/checkpoint"
        model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
            filepath=checkpoint_path,
            save_weights_only=True,
            monitor='val_top_k_categorical_accuracy',
            mode='max',
            save_best_only=True
        )

        # è®­ç»ƒ
        if config["use_dp_condition"] or config["dp_concatenate"]:
            history = model.fit(
                [train_x_gps, train_x_dp, train_x_extra], train_y,
                epochs=200, batch_size=128,
                validation_data=([val_x_gps, val_x_dp, val_x_extra], val_y),
                callbacks=[model_checkpoint],
                verbose=1
            )
        else:
            history = model.fit(
                [train_x_gps, train_x_extra], train_y,
                epochs=200, batch_size=128,
                validation_data=([val_x_gps, val_x_extra], val_y),
                callbacks=[model_checkpoint],
                verbose=1
            )

        print(f"Finished training {config_name}")


if __name__ == "__main__":
    # ç¡®ä¿æ£€æŸ¥ç‚¹ç›®å½•å­˜åœ¨
    os.makedirs("ablation_checkpoints", exist_ok=True)

    # åªè¿è¡Œéfullçš„é…ç½®
    non_full_configs = {k: v for k, v in ablation_configs.items() if k != "CBAM_full"}

    # ä¸´æ—¶æ›¿æ¢é…ç½®
    original_configs = ablation_configs.copy()
    ablation_configs.clear()
    ablation_configs.update(non_full_configs)

    print("Running non-full ablation configurations:")
    for config_name in ablation_configs.keys():
        print(f"  - {config_name}")

    # è¿è¡Œéfullçš„æ¶ˆèå®éªŒ
    results = run_ablation_study()

    # æ¢å¤å®Œæ•´é…ç½®ï¼ˆå¯é€‰ï¼‰
    ablation_configs.clear()
    ablation_configs.update(original_configs)

    print("\nğŸ‰ Non-full ablation study completed!")
